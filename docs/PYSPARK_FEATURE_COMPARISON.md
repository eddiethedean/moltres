# PySpark Feature Comparison

This document provides a comprehensive comparison of PySpark DataFrame API features against all 6 Moltres interfaces:

1. **PySpark-Style (Sync)** - `DataFrame` - Primary PySpark-compatible API
2. **PySpark-Style (Async)** - `AsyncDataFrame` - Async version of PySpark-style API
3. **Pandas-Style (Sync)** - `PandasDataFrame` - Pandas-compatible API
4. **Pandas-Style (Async)** - `AsyncPandasDataFrame` - Async version of Pandas-style API
5. **Polars-Style (Sync)** - `PolarsDataFrame` - Polars LazyFrame-compatible API
6. **Polars-Style (Async)** - `AsyncPolarsDataFrame` - Async version of Polars-style API

## Status Indicators

- âœ… **Supported** - Fully implemented with full feature parity
- âš ï¸ **Partial** - Partially implemented or with limitations
- âŒ **Not Implemented** - Not available in this interface
- ğŸ”„ **Different API** - Available but with different method name/API signature
- ğŸ“ **Notes** - Additional implementation details, differences, or limitations

## Selection & Projection

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `select(*cols)` | âœ… | âœ… | âœ… (`select()`) | âœ… (`select()`) | âœ… | âœ… | All interfaces support column selection |
| `selectExpr(*exprs)` | âœ… | âœ… | âœ… (`select_expr()`) | âœ… (`select_expr()`) | âœ… (`select_expr()`) | âœ… (`select_expr()`) | SQL expression selection |
| Column access `df.col` | âœ… (`__getattr__`) | âœ… (`__getattr__`) | ğŸ”„ (`df['col']`) | ğŸ”„ (`df['col']`) | ğŸ”„ (`df['col']`) | ğŸ”„ (`df['col']`) | PySpark-style supports dot notation; Pandas/Polars use bracket notation `df['col']` instead |
| `df["col"]` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Pandas/Polars-style column access (bracket notation) |
| `df[["col1", "col2"]]` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Multi-column selection (Pandas/Polars) |
| `df[df["col"] > 5]` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Boolean indexing (Pandas/Polars) |

## Filtering

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `where(condition)` | âœ… | âœ… | ğŸ”„ (`query()`) | ğŸ”„ (`query()`) | âœ… (`filter()`) | âœ… (`filter()`) | All support filtering |
| `filter(condition)` | âœ… | âœ… | ğŸ”„ (`query()`) | ğŸ”„ (`query()`) | âœ… | âœ… | Alias for `where()` in PySpark-style |
| `query(expr)` | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | Pandas-style string query syntax |
| `isin(values)` | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | Pandas-style membership check |
| `between(start, end)` | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | Pandas-style range check |

## Joins

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `join(other, on, how)` | âœ… | âœ… | âœ… (`merge()`) | âœ… (`merge()`) | âœ… | âœ… | All support joins |
| `crossJoin(other)` | âœ… | âœ… | âŒ | âŒ | âœ… (`cross_join()`) | âœ… (`cross_join()`) | Cross join support |
| `semi_join(other, on)` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Semi-join (filter rows with matches) |
| `anti_join(other, on)` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Anti-join (filter rows without matches) |
| Join types: `inner`, `left`, `right`, `outer` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | All join types supported |

## GroupBy & Aggregations

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `groupBy(*cols)` | âœ… | âœ… | âœ… (`groupby()`) | âœ… (`groupby()`) | âœ… (`group_by()`) | âœ… (`group_by()`) | All support grouping |
| `agg(*exprs)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Aggregation expressions |
| `agg({"col": "func"})` | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | Dictionary syntax (PySpark/Pandas) |
| `sum()`, `mean()`, `min()`, `max()`, `count()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Basic aggregations |
| `first()`, `last()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | First/last value per group |
| `nunique()` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Count distinct (Pandas/Polars) |
| `std()`, `var()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Statistical aggregations |
| `pivot(pivot_col)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Pivot operation |

## Sorting

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `orderBy(*cols)` | âœ… | âœ… | âœ… (`sort_values()`) | âœ… (`sort_values()`) | âœ… (`sort()`) | âœ… (`sort()`) | All support sorting |
| `sort(*cols)` | âœ… | âœ… | âœ… (`sort_values()`) | âœ… (`sort_values()`) | âœ… | âœ… | Alias for `orderBy()` |
| `sort_values(by, ascending)` | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | Pandas-style sorting |
| `sort(by, descending)` | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | Polars-style sorting |

## Window Functions

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `over(windowSpec)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Window function support |
| `row_number()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Row number window function |
| `rank()`, `dense_rank()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Ranking functions |
| `lead()`, `lag()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Lead/lag functions |
| `first_value()`, `last_value()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | First/last value in window |

## Set Operations

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `union(other)` | âœ… | âœ… | âœ… (`concat()`) | âœ… (`concat()`) | âœ… | âœ… | Union operation |
| `unionAll(other)` | âœ… | âœ… | âœ… (`concat()`) | âœ… (`concat()`) | âœ… | âœ… | Union all (same as union) |
| `intersect(other)` | âœ… | âœ… | âœ… (`concat()` + dedup) | âœ… (`concat()` + dedup) | âœ… | âœ… | Intersection |
| `except(other)` | âœ… | âœ… | âœ… (`concat()` + filter) | âœ… (`concat()` + filter) | âœ… (`difference()`) | âœ… (`difference()`) | Set difference |
| `distinct()` | âœ… | âœ… | âœ… (`drop_duplicates()`) | âœ… (`drop_duplicates()`) | âœ… | âœ… | Remove duplicates |
| `dropDuplicates(subset)` | âœ… | âœ… | âœ… | âœ… | âœ… (`unique()`) | âœ… (`unique()`) | Drop duplicates with subset |

## Column Manipulation

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `withColumn(name, expr)` | âœ… | âœ… | âœ… (`assign()`) | âœ… (`assign()`) | âœ… (`with_column()`) | âœ… (`with_column()`) | Add/replace column |
| `withColumns({name: expr})` | âŒ | âŒ | âœ… (`assign()`) | âœ… (`assign()`) | âœ… (`with_columns()`) | âœ… (`with_columns()`) | Multiple columns |
| `drop(*cols)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Drop columns |
| `withColumnRenamed(old, new)` | âœ… | âœ… | âœ… (`rename()`) | âœ… (`rename()`) | âœ… (`rename()`) | âœ… (`rename()`) | Rename column |
| `alias(name)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Column alias |
| `assign(**kwargs)` | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | Pandas-style column assignment |

## Null Handling

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `fillna(value)` | âœ… | âœ… | âœ… | âœ… | âœ… (`fill_null()`) | âœ… (`fill_null()`) | Fill null values |
| `fillna({col: value})` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Column-specific fill |
| `dropna(how, subset)` | âœ… | âœ… | âœ… | âœ… | âœ… (`drop_nulls()`) | âœ… (`drop_nulls()`) | Drop null rows |
| `na.drop()` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Null handling property |
| `na.fill(value)` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Null handling property |

## String Operations

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `df["col"].str.upper()` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String accessor |
| `df["col"].str.lower()` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String accessor |
| `df["col"].str.contains(pattern)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String contains |
| `df["col"].str.startswith(pattern)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String startswith |
| `df["col"].str.endswith(pattern)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String endswith |
| `df["col"].str.replace(old, new)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String replace |
| `df["col"].str.split(delimiter)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String split |
| `df["col"].str.len()` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | String length |
| `upper(col)`, `lower(col)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | String functions (all) |
| `substring(col, pos, len)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Substring function |

## Date/Time Operations

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `df["col"].dt.year` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Date accessor |
| `df["col"].dt.month` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Date accessor |
| `df["col"].dt.day` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Date accessor |
| `df["col"].dt.hour` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Date accessor |
| `year(col)`, `month(col)`, etc. | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Date functions (all) |
| `to_date(col)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Date conversion |
| `to_timestamp(col)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Timestamp conversion |
| `date_add(col, days)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Date arithmetic |
| `date_sub(col, days)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Date arithmetic |

## File I/O

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `spark.read.csv(path)` | âœ… (`db.read.csv()`) | âœ… (`db.read.csv()`) | âŒ | âŒ | âœ… (`db.scan_csv()`) | âœ… (`db.scan_csv()`) | Read CSV |
| `spark.read.json(path)` | âœ… (`db.read.json()`) | âœ… (`db.read.json()`) | âŒ | âŒ | âœ… (`db.scan_json()`) | âœ… (`db.scan_json()`) | Read JSON |
| `spark.read.parquet(path)` | âœ… (`db.read.parquet()`) | âœ… (`db.read.parquet()`) | âŒ | âŒ | âœ… (`db.scan_parquet()`) | âœ… (`db.scan_parquet()`) | Read Parquet |
| `spark.read.text(path)` | âœ… (`db.read.text()`) | âœ… (`db.read.text()`) | âŒ | âŒ | âœ… (`db.scan_text()`) | âœ… (`db.scan_text()`) | Read text |
| `df.write.csv(path)` | âœ… | âœ… | âŒ | âŒ | âœ… (`write_csv()`) | âœ… (`write_csv()`) | Write CSV |
| `df.write.json(path)` | âœ… | âœ… | âŒ | âŒ | âœ… (`write_json()`) | âœ… (`write_json()`) | Write JSON |
| `df.write.parquet(path)` | âœ… | âœ… | âŒ | âŒ | âœ… (`write_parquet()`) | âœ… (`write_parquet()`) | Write Parquet |
| `df.write.saveAsTable(name)` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Save to table |
| `df.write.insertInto(table)` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Insert into table |
| `df.write.mode("overwrite")` | âœ… | âœ… | âŒ | âŒ | âœ… | âœ… | Write mode |

## Schema Operations

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `df.columns` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Column names |
| `df.schema` | âœ… | âœ… | ğŸ”„ (`dtypes`) | ğŸ”„ (`dtypes`) | âœ… | âœ… | Schema information |
| `df.dtypes` | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | Column types (Pandas-style) |
| `df.printSchema()` | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | Print schema tree |
| `df.schema` (Polars) | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | Polars schema format |

## Execution Methods

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `collect()` | âœ… | âœ… (`await collect()`) | âœ… | âœ… (`await collect()`) | âœ… | âœ… (`await collect()`) | Execute and return results |
| `collect()` (streaming) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Streaming execution |
| `show(n, truncate)` | âœ… | âœ… (`await show()`) | âŒ | âŒ | âŒ | âŒ | Display results |
| `take(n)` | âœ… | âœ… (`await take()`) | âŒ | âŒ | âœ… (`fetch()`) | âœ… (`await fetch()`) | Take n rows |
| `first()` | âœ… | âœ… (`await first()`) | âŒ | âŒ | âŒ | âŒ | First row |
| `head(n)` | âœ… | âœ… (`await head()`) | âœ… | âœ… | âœ… | âœ… | First n rows |
| `tail(n)` | âœ… | âœ… (`await tail()`) | âœ… | âœ… | âœ… | âœ… | Last n rows |
| `count()` | âœ… | âœ… (`await count()`) | âŒ | âŒ | âŒ | âŒ | Row count |
| `fetch(n)` (Polars) | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… (`await fetch()`) | Polars-style fetch |

## Statistics & Descriptive

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `describe(*cols)` | âœ… | âœ… (`await describe()`) | âœ… | âœ… (`await describe()`) | âœ… (`await describe()`) | âœ… (`await describe()`) | Statistical summary |
| `summary(*stats)` | âœ… | âœ… (`await summary()`) | âŒ | âŒ | âŒ | âŒ | Custom statistics |
| `nunique(column)` | âŒ | âŒ | âœ… | âœ… (`await nunique()`) | âœ… | âœ… | Count unique values |
| `value_counts(column)` | âŒ | âŒ | âœ… | âœ… (`await value_counts()`) | âŒ | âŒ | Value frequency (Pandas) |
| `info()` | âŒ | âŒ | âœ… | âœ… (`await info()`) | âŒ | âŒ | DataFrame info (Pandas) |
| `empty` | âŒ | âŒ | âœ… | âœ… (`await empty`) | âŒ | âŒ | Check if empty (Pandas) |
| `shape` | âŒ | âŒ | âœ… | âœ… (`await shape`) | âŒ | âŒ | DataFrame shape (Pandas) |
| `width`, `height` (Polars) | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… (`await height`) | Polars dimensions |

## Data Reshaping

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `pivot(pivot_col, values)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Pivot operation |
| `explode(col)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Explode array/JSON |
| `melt(id_vars, value_vars)` | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | Unpivot (Pandas/Polars) |
| `unnest(cols)` | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | Unnest nested structures (Polars) |
| `slice(offset, length)` | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | Slice rows (Polars) |

## Sampling & Limiting

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `sample(fraction, seed)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Random sampling |
| `limit(n)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Limit rows |

## CTEs & SQL

| PySpark Method | PySpark-Style (Sync) | PySpark-Style (Async) | Pandas-Style (Sync) | Pandas-Style (Async) | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------------|---------------------|----------------------|-------------------|---------------------|-------------------|---------------------|-------|
| `cte(name)` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Common Table Expression |
| `with_recursive(name)` | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | Recursive CTE (Polars) |
| `spark.sql(query)` | âœ… (`db.sql()`) | âœ… (`await db.sql()`) | âŒ | âŒ | âŒ | âŒ | Raw SQL execution |
| `to_sql()` | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | Get SQL string |
| `to_sqlalchemy()` | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Get SQLAlchemy statement |

## Interface-Specific Features

### Pandas-Style Unique Features

| Feature | Pandas-Style (Sync) | Pandas-Style (Async) | Notes |
|---------|-------------------|---------------------|-------|
| `query(expr)` | âœ… | âœ… | String-based query syntax |
| `loc` indexer | âœ… | âœ… (`await loc`) | Label-based indexing |
| `iloc` indexer | âœ… | âœ… (`await iloc`) | Integer-based indexing |
| `append(other)` | âœ… | âœ… | Append DataFrames |
| `isin(values)` | âœ… | âœ… | Membership check |
| `between(start, end)` | âœ… | âœ… | Range check |
| `assign(**kwargs)` | âœ… | âœ… | Column assignment |
| `sort_values(by, ascending)` | âœ… | âœ… | Sorting with parameters |
| `value_counts(column)` | âœ… | âœ… | Value frequency |
| `info()` | âœ… | âœ… | DataFrame info |
| `melt()` | âœ… | âœ… | Unpivot operation |

### Polars-Style Unique Features

| Feature | Polars-Style (Sync) | Polars-Style (Async) | Notes |
|---------|-------------------|---------------------|-------|
| `lazy()` | âœ… | âœ… | Mark as lazy (no-op in Moltres) |
| `fetch(n)` | âœ… | âœ… (`await fetch()`) | Fetch n rows |
| `with_columns(*exprs)` | âœ… | âœ… | Multiple column operations |
| `with_columns_renamed(mapping)` | âœ… | âœ… | Rename multiple columns |
| `with_row_count(name)` | âœ… | âœ… | Add row number column |
| `with_context(df)` | âœ… | âœ… | Add context DataFrame |
| `with_recursive(name)` | âœ… | âœ… | Recursive CTE |
| `unnest(cols)` | âœ… | âœ… | Unnest nested structures |
| `slice(offset, length)` | âœ… | âœ… | Slice rows |
| `gather_every(n, offset)` | âœ… | âœ… | Sample every nth row |
| `interpolate(method)` | âœ… | âœ… | Interpolate missing values |
| `quantile(quantile)` | âœ… | âœ… | Quantile calculation |
| `hstack(other)` | âœ… | âœ… | Horizontal stack |
| `vstack(other)` | âœ… | âœ… | Vertical stack |
| `difference(other)` | âœ… | âœ… | Set difference |
| `cross_join(other)` | âœ… | âœ… | Cross join |
| `drop_nulls(subset)` | âœ… | âœ… | Drop nulls |
| `fill_null(value)` | âœ… | âœ… | Fill nulls |
| `unique(subset)` | âœ… | âœ… | Unique rows |
| `explain(format)` | âœ… | âœ… (`await explain()`) | Query plan explanation |

### Async-Specific Features

| Feature | PySpark-Style (Async) | Pandas-Style (Async) | Polars-Style (Async) | Notes |
|---------|---------------------|---------------------|---------------------|-------|
| `await collect()` | âœ… | âœ… | âœ… | Async execution |
| `await collect(stream=True)` | âœ… | âœ… | âœ… | Async streaming |
| `await show()` | âœ… | âŒ | âŒ | Async display |
| `await take()` | âœ… | âŒ | âœ… (`await fetch()`) | Async take |
| `await first()` | âœ… | âŒ | âŒ | Async first row |
| `await head()` | âœ… | âœ… | âœ… | Async head |
| `await tail()` | âœ… | âœ… | âœ… | Async tail |
| `await count()` | âœ… | âŒ | âŒ | Async count |
| `await describe()` | âœ… | âœ… | âœ… | Async describe |
| `await summary()` | âœ… | âŒ | âŒ | Async summary |
| `await nunique()` | âŒ | âœ… | âœ… | Async nunique |
| `await value_counts()` | âŒ | âœ… | âŒ | Async value_counts |
| `await info()` | âŒ | âœ… | âŒ | Async info |
| `await shape` | âŒ | âœ… | âŒ | Async shape |
| `await empty` | âŒ | âœ… | âŒ | Async empty |
| `await height` | âŒ | âŒ | âœ… | Async height |
| `await schema` | âŒ | âŒ | âœ… | Async schema |
| `await dtypes` | âŒ | âœ… | âŒ | Async dtypes |
| `await fetch()` | âŒ | âŒ | âœ… | Async fetch |
| `await write_csv()` | âŒ | âŒ | âœ… | Async write CSV |
| `await write_json()` | âŒ | âŒ | âœ… | Async write JSON |
| `await write_parquet()` | âŒ | âŒ | âœ… | Async write Parquet |
| `await explain()` | âŒ | âŒ | âœ… | Async explain |
| `await loc` | âŒ | âœ… | âŒ | Async loc indexer |
| `await iloc` | âŒ | âœ… | âŒ | Async iloc indexer |

## Summary

### Overall Coverage

- **PySpark-Style (Sync)**: ~98% API compatibility with PySpark DataFrame API
- **PySpark-Style (Async)**: Full async support for all sync methods
- **Pandas-Style (Sync)**: Comprehensive Pandas DataFrame API with SQL pushdown
- **Pandas-Style (Async)**: Full async support for all Pandas-style methods
- **Polars-Style (Sync)**: Comprehensive Polars LazyFrame API with SQL pushdown
- **Polars-Style (Async)**: Full async support for all Polars-style methods

### Key Differences

1. **Column Access**: PySpark uses `df.col` attribute access, while Pandas/Polars use `df['col']` bracket notation
2. **Filtering**: PySpark uses `where()`/`filter()`, Pandas uses `query()`, Polars uses `filter()`
3. **GroupBy**: PySpark uses `groupBy()`, Pandas uses `groupby()`, Polars uses `group_by()`
4. **Sorting**: PySpark uses `orderBy()`, Pandas uses `sort_values()`, Polars uses `sort()`
5. **Null Handling**: PySpark uses `na` property, Pandas/Polars use direct methods
6. **File I/O**: PySpark uses `spark.read.*`, Moltres uses `db.read.*` or `db.scan_*` (Polars)
7. **Async**: All async interfaces require `await` for execution methods

### Implementation Notes

- All interfaces maintain lazy evaluation until execution
- SQL pushdown execution for all operations
- Type safety with proper type hints
- Comprehensive error handling and validation
- Support for multiple database dialects (SQLite, PostgreSQL, MySQL, DuckDB)

